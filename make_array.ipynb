{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to JupyROOT 6.14/06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from ROOT import (TCanvas, TPad, TFile, TPaveLabel, \n",
    "                  TPaveText, gROOT, TH1F, TH1D, TLegend, \n",
    "                  gStyle, TH2F, TChain, TGraphErrors, TText, gPad, gROOT, TTree)\n",
    "from array import array\n",
    "import datetime\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(42)\n",
    "        \n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Concatenate, Dense, LSTM, Input, concatenate, SimpleRNN, GRU, Masking, Lambda, Reshape, Dropout, RNN\n",
    "from keras.optimizers import Adagrad, SGD, RMSprop, Adam\n",
    "\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, LeakyReLU, LSTM, Conv1D, SimpleRNN, Concatenate\n",
    "from keras import optimizers\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "import keras.backend as K\n",
    "import pickle\n",
    "#import seaborn as sns\n",
    "\n",
    "TODAY = datetime.datetime.now().strftime('%Y-%m-%d')\n",
    "\n",
    "def init_dirs():\n",
    "    import datetime, os\n",
    "    for _dir in [\"plots\", \"models\"]:\n",
    "        today_dir = os.path.join(_dir, TODAY)\n",
    "        if not os.path.isdir(today_dir):\n",
    "            os.makedirs(today_dir)\n",
    "            \n",
    "#init_dirs()\n",
    "from sklearn import preprocessing\n",
    "min_max_scaler = preprocessing.MinMaxScaler()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "have 24 features\n",
      "mZp is MZ1\n"
     ]
    }
   ],
   "source": [
    "# all methods here\n",
    "def clean_arrays(data, weights=None):\n",
    "    new = []\n",
    "    for d in data:\n",
    "        #if all(di == 0. for di in d):\n",
    "        if d[-1]==0.:\n",
    "            continue\n",
    "        new.append(d)\n",
    "    out = np.array(new)\n",
    "    print(\"before\", data.shape, 'after', out.shape)\n",
    "    return out\n",
    "\n",
    "def getweight(wt, Xsec):\n",
    "    #totalWt = sum(wt)\n",
    "    wt = Xsec*139*wt\n",
    "    return wt\n",
    "\n",
    "def build_array(fname, features, cuts, mll_idx = 10, should_clean_arrays=True, **kwargs):\n",
    "    #print(features[mll_idx])\n",
    "    f = TFile.Open(fname)\n",
    "    tree = f.tree_NOMINAL\n",
    "    n_events = tree.GetEntries()\n",
    "    data = np.zeros((n_events, len(features)))\n",
    "    for n, (event, element) in enumerate(zip(tree, data)):\n",
    "        for i, feature in enumerate(features):\n",
    "            try:\n",
    "                val = getattr(event, feature)\n",
    "            except:\n",
    "                val = None\n",
    "            if feature in cuts and eval(\"{}{}\".format(val, cuts[feature])):\n",
    "                element[i] = getattr(event, feature)\n",
    "            elif feature in cuts:\n",
    "                element = np.zeros(len(features))\n",
    "                break\n",
    "            elif feature == 'train_mass':\n",
    "                element[i] = getattr(event, features[mll_idx])\n",
    "            elif feature == 'MZ1_MZ2':\n",
    "                element[i] = getattr(event, 'MZ1')-getattr(event, 'MZ2')\n",
    "            else:\n",
    "                element[i] = val\n",
    "    return clean_arrays(data) if should_clean_arrays else data\n",
    "\n",
    "features = ('train_mass', ## 0\n",
    "            'PtL1',       ## 1\n",
    "            'PtL2',       ## 2\n",
    "            'PtL3',       ## 3\n",
    "            'PtL4',       ## 4\n",
    "            'EtaL1',      ## 5\n",
    "            'EtaL2',      ## 6\n",
    "            'EtaL3',      ## 7\n",
    "            'EtaL4',      ## 8\n",
    "            'MZ1',        ## 9\n",
    "            'MZ2',        ## 10\n",
    "            'MZ1_MZ2',    ## 11\n",
    "            'PtZ1',       ## 12\n",
    "            'PtZ2',       ## 13\n",
    "            'MZZ',        ## 14\n",
    "            'PtZZ',       ## 15\n",
    "            'DeltaRl12',  ## 16\n",
    "            'DeltaRl34',  ## 17\n",
    "            'dEtal12',    ## 18\n",
    "            'dEtal34',    ## 19\n",
    "            'run',        ## 20\n",
    "            'event',      ## 21\n",
    "            'quadtype',   ## 22\n",
    "            'weight')     ## 23\n",
    "\n",
    "# these are your cuts that preselect events, empty now\n",
    "#cuts = dict()\n",
    "#cuts = {\"MZZ\": \"<120\"}\n",
    "cuts = {\"quadtype\": \"==2\",\n",
    "        \"MZZ\": \"<180\"}\n",
    "\n",
    "def unison_shuffled_copies(*arr):\n",
    "    assert all(len(a) for a in arr)\n",
    "    p = np.random.permutation(len(arr[0]))\n",
    "    return (a[p] for a in arr)\n",
    "\n",
    "mll_idx = 9 # 10 when signal sample < 40GeV; 9 when signal >40GeV\n",
    "\n",
    "print(\"have {} features\".format(len(features)))\n",
    "print(\"mZp is\", features[mll_idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loop over samples and create skimmed files only containing interesting features\n",
    "\n",
    "Make sure all directories are correct. In this case I save my train and test data to ``example_hep/train`` and ``example_hep/test``. I only use the files in ``train`` for training and for evaluation I only use ``test``.\n",
    "\n",
    "The msamp list contains the sample number and masses.\n",
    "\n",
    "## So first for the signal samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before (4493, 24) after (4490, 24)\n",
      "Training, validation, testing:  (2694, 24) (898, 24) (898, 24)\n",
      "before (4496, 24) after (4493, 24)\n",
      "Training, validation, testing:  (2695, 24) (899, 24) (899, 24)\n",
      "before (4256, 24) after (4254, 24)\n",
      "Training, validation, testing:  (2552, 24) (851, 24) (851, 24)\n",
      "before (4175, 24) after (4172, 24)\n",
      "Training, validation, testing:  (2503, 24) (834, 24) (835, 24)\n",
      "before (3800, 24) after (3797, 24)\n",
      "Training, validation, testing:  (2278, 24) (759, 24) (760, 24)\n",
      "before (6859, 24) after (6851, 24)\n",
      "Training, validation, testing:  (4110, 24) (1370, 24) (1371, 24)\n",
      "before (5792, 24) after (5786, 24)\n",
      "Training, validation, testing:  (3471, 24) (1157, 24) (1158, 24)\n",
      "before (9519, 24) after (9513, 24)\n",
      "Training, validation, testing:  (5707, 24) (1903, 24) (1903, 24)\n",
      "before (7567, 24) after (7561, 24)\n",
      "Training, validation, testing:  (4536, 24) (1512, 24) (1513, 24)\n",
      "before (4757, 24) after (4750, 24)\n",
      "Training, validation, testing:  (2850, 24) (950, 24) (950, 24)\n",
      "before (4035, 24) after (4033, 24)\n",
      "Training, validation, testing:  (2419, 24) (807, 24) (807, 24)\n",
      "before (3700, 24) after (3695, 24)\n",
      "Training, validation, testing:  (2217, 24) (739, 24) (739, 24)\n"
     ]
    }
   ],
   "source": [
    "import io\n",
    "import numpy as np\n",
    "\n",
    "msamp = [42,45,48,51,54,57,60,63,66,69,72,75]\n",
    "\n",
    "## Need to do rescale to only training datasets\n",
    "## firstly sum them together to fit transform\n",
    "#raw_bkg1 = build_array(\n",
    "#    '/Users/zhuheling/ML/keras/example_hep/myTest/data/tree_bk.root',\n",
    "#    features, cuts, mll_idx=10)\n",
    "#raw_train = raw_bkg1[0:int(len(raw_bkg1)*0.6)]\n",
    "#raw_train[:,0] = 40\n",
    "#print (np.amax(raw_train, axis=0), np.amin(raw_train, axis=0))\n",
    "#for mass in msamp:\n",
    "#    if mass<40:\n",
    "#        m_idx=10\n",
    "#    else:\n",
    "#        m_idx=9\n",
    "#    raw_signal_add = build_array(\n",
    "#        '/Users/zhuheling/ML/keras/example_hep/myTest/data/tree_{}GeV.root'.format(mass), \n",
    "#        features, cuts, m_idx)\n",
    "#    raw_train_add = raw_signal_add[0:int(len(raw_signal_add)*0.6)]\n",
    "#    raw_train = np.concatenate((raw_train,raw_train_add))\n",
    "#print (np.amax(raw_train, axis=0), np.amin(raw_train, axis=0))\n",
    "\n",
    "#min_max_scaler.fit_transform(raw_train[:,0:20])\n",
    "\n",
    "for mass in msamp:\n",
    "    if mass<40:\n",
    "        m_idx=10\n",
    "    else:\n",
    "        m_idx=9\n",
    "    signal = build_array(\n",
    "        '/Users/zhuheling/ML/keras/example_hep/myTest/data/tree_{}GeV.root'.format(mass), \n",
    "        features, cuts, m_idx)\n",
    "    #signal = np.zeros( (len(raw_signal), 43))\n",
    "    #signal[:,0:20] = min_max_scaler.transform(raw_signal[:,0:20])\n",
    "    #signal[:,20:40] = raw_signal[:,0:20]\n",
    "    #signal[:,40:43] = raw_signal[:,20:23]\n",
    "    #print(raw_signal[0:3])\n",
    "    #print(signal[0:3])\n",
    "   \n",
    "    with io.open('/Users/zhuheling/ML/keras/example_hep/myTest/Below180/UnscaledData/data_npy/tree_{}GeV.npy'.format(mass), 'wb') as f:\n",
    "        np.save(f, signal)\n",
    "\n",
    "    n_trains = int(len(signal)*0.6)\n",
    "    n_vals = int(len(signal)*0.8)\n",
    "    \n",
    "    strain = signal[0:n_trains]\n",
    "    with io.open('/Users/zhuheling/ML/keras/example_hep/myTest/Below180/UnscaledData/train/tree_{}GeV.npy'.format(mass), 'wb') as f:\n",
    "        np.save(f, strain)\n",
    "    \n",
    "    sval = signal[n_trains:n_vals]\n",
    "    with io.open('/Users/zhuheling/ML/keras/example_hep/myTest/Below180/UnscaledData/Validation/tree_{}GeV.npy'.format(mass), 'wb') as f:\n",
    "        np.save(f, sval)\n",
    "    \n",
    "    stest = signal[n_vals:]\n",
    "    with io.open('/Users/zhuheling/ML/keras/example_hep/myTest/Below180/UnscaledData/test/tree_{}GeV.npy'.format(mass), 'wb') as f:\n",
    "        np.save(f, stest)\n",
    "        \n",
    "    print (\"Training, validation, testing: \", strain.shape, sval.shape, stest.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# And now for the background samples\n",
    "Need to re-define the mass parameter (train_mass) for backgrounds used for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before (52953, 24) after (22918, 24)\n",
      "Training, validation, testing:  (13750, 24) (4584, 24) (4584, 24)\n",
      "Weighted training, validation, testing:  568.2608611656711 191.02904621943267 189.81990211790256\n"
     ]
    }
   ],
   "source": [
    "import io\n",
    "import numpy as np\n",
    "\n",
    "bkg1 = build_array(\n",
    "    '/Users/zhuheling/ML/keras/example_hep/myTest/data/tree_qqZZ.root',\n",
    "    features, cuts, mll_idx=9)\n",
    "\n",
    "#print(raw_bkg1[0:3])\n",
    "#bkg1 = np.zeros( (len(raw_bkg1), 43))\n",
    "#bkg1[:,0:20] = min_max_scaler.transform(raw_bkg1[:,0:20])\n",
    "#bkg1[:,20:40] = raw_bkg1[:,0:20]\n",
    "#bkg1[:,40:43] = raw_bkg1[:,20:23]\n",
    "#print(bkg1[0:3])\n",
    "\n",
    "# Xsec: ggZZ: 10.163 fb; qqZZ: 1252 fb; ggZZ_low: 9.958 fb\n",
    "# Sumofweight: ggZZ: 1176146.3; qqZZ: 5043806; ggZZ_low: 100064.62\n",
    "#print(bkg1[0:5,-1])\n",
    "bkg1[:,-1] = getweight(bkg1[:,-1], 1252./5043806.)\n",
    "#print(bkg1[0:5,-1])\n",
    "with io.open('/Users/zhuheling/ML/keras/example_hep/myTest/Below180/UnscaledData/data_npy/tree_qqZZ.npy', 'wb') as f:\n",
    "        np.save(f, bkg1)\n",
    "\n",
    "n_trains = int(len(bkg1)*0.6)\n",
    "n_vals = int(len(bkg1)*0.8)\n",
    "\n",
    "btrain = bkg1[0:n_trains]\n",
    "with io.open('/Users/zhuheling/ML/keras/example_hep/myTest/Below180/UnscaledData/train/tree_qqZZ.npy', 'wb') as f:\n",
    "    np.save(f, btrain)\n",
    "    \n",
    "bval = bkg1[n_trains:n_vals]\n",
    "with io.open('/Users/zhuheling/ML/keras/example_hep/myTest/Below180/UnscaledData/Validation/tree_qqZZ.npy', 'wb') as f:\n",
    "    np.save(f, bval)\n",
    "    \n",
    "btest = bkg1[n_vals:]\n",
    "with io.open('/Users/zhuheling/ML/keras/example_hep/myTest/Below180/UnscaledData/test/tree_qqZZ.npy', 'wb') as f:\n",
    "    np.save(f, btest)\n",
    "    \n",
    "print (\"Training, validation, testing: \", btrain.shape, bval.shape, btest.shape)\n",
    "print (\"Weighted training, validation, testing: \", sum(btrain[:,-1]), sum(bval[:,-1]), sum(btest[:,-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
